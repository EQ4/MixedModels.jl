{
 "metadata": {
  "language": "Julia",
  "name": "",
  "signature": "sha256:1f74c4947369d8a8c6364635122412de1230796bd9abdff2cc53eeb35941682b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Fitting GLMs in Julia"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One of the oldest techniques in statistics is fitting _linear models_, which encompass various regression models and the models underlying the _analysis of variance_.  In the mid-20'th century the practice of fitting a response by least squares was extended to provide for models like _logistic regression_, for a binary response or for binomial response data, and _Poisson regression_, when the response is a count.\n",
      "\n",
      "In 1972 Nelder and Wedderburn published a paper [\"Generalized Linear Models\"](http://en.wikipedia.org/wiki/Generalized_linear_model) in J. Royal Statistical Society, Series A that provided a unifying framework for these and other models.  In these models a response, which can be binary or integer-valued or real-valued, is measured on $n$ occasions along with the values of _covariates_, which are also called \"predictor\" variables.\n",
      "\n",
      "## Components of the model\n",
      "\n",
      "The model is a description of the probability distribution of the response given values of parameters.  From the probability model and the observed data, both the response and the covariates, we estimate values of parameters.\n",
      "\n",
      "The values of the covariates and the model form determine a _model matrix_, $\\bf X$.  A _linear predictor_ is of the form $\\bf\\eta=X\\beta$, where $\\bf\\beta$ is the _coefficient vector_ to be estimated.  In some cases there is an additional _scale parameter_ to be estimated.\n",
      "\n",
      "### Linear regression\n",
      "\n",
      "The probability model for linear regression is\n",
      "$$ \\mathcal{Y}\\sim\\mathcal{N}(X\\beta,\\rm\\sigma^2\\bf I)$$\n",
      "\n",
      "In this case, the _mean response_ is the linear predictor.  That is, $$\\rm E[\\mathcal{Y}]=\\bf\\mu=\\eta=\\bf X\\beta$$\n",
      "and the distribution of the response is multivariate normal (or Gaussian) with scale parameter, $\\sigma$.  Furthermore, the responses are independent.\n",
      "\n",
      "### A link function\n",
      "\n",
      "One characteristic of a linear predictor is that its range is a complete linear subspace of the $n$-dimensional response space.  For a normal distribution there are no constraints on the expected response, which makes it meaningful for us to set $\\bf\\mu=\\eta$.  For other distributions the range of the expected response is constrained.  For a Bernoulli distribution we have $0\\le\\mu_i\\le1,i=1,\\dots,n$ and for a Poisson we have $0<\\mu_i$.\n",
      "\n",
      "To accomodate these constraints we map the linear predictor values, $\\eta_i$, to the expected responses, $\\mu_i$, in such a way that the constraints hold.  For historical reasons it is the inverse map, $g:\\mu_i\\rightarrow\\eta_i$, that is called the _link function_.  In practice we use the inverse link function, $g^{-1}:\\eta_i\\rightarrow\\mu_i$ more than we use the link.\n",
      "\n",
      "### Canonical link functions\n",
      "\n",
      "If all we require in a link function is that the range of the inverse link is the appropriate interval for the expected value, i.e. $[0,1]$ for Bernoulli responses and $(0,\\infty)$ for Poisson responses, then we have a very wide range of functions to choose from.  However, Nelder and Wedderburn showed that a _canonical link function_ can be derived from the probability mass function or probability density function for distributions that are in the [exponential family](http://en.wikipedia.org/wiki/Exponential_family).  The derivation is not particularly difficult but we won't include it here.\n",
      "\n",
      "The most common examples of canonical link functions are:\n",
      "- the _logit_ or \"log odds\" link for the Bernoulli distribution\n",
      "$$g(\\mu)=\\log\\left(\\frac{\\mu}{1-\\mu}\\right),\\quad0<\\mu<1$$\n",
      "- the _log_ link for the Poisson distribution\n",
      "$$g(\\mu)=\\log(\\mu),\\quad0<\\mu$$\n",
      "\n",
      "Interestingly, [logistic regression](http://en.wikipedia.org/wiki/Logistic_regression) is named after the inverse link function\n",
      "$$g^{-1}(\\eta)=\\frac{1}{1+\\exp(-\\eta)}$$\n",
      "which is called the logistic function.\n",
      "\n",
      "### Relating the variance to the mean\n",
      "\n",
      "The normal distribution has many, many interesting properties, one of which is that the location (mean) and spread (standard deviation) can be controlled separately.  This is not true for most distributions.  For many distributions like the Bernoulli or the Poisson the variance or standard deviation is a function of the mean.\n",
      "\n",
      "## Specification of a GLM\n",
      "\n",
      "To specify a generalized linear model we must provide\n",
      "- the observed responses, $\\bf y$\n",
      "- the model matrix, $\\bf X$, derived from the values of the covariates\n",
      "- the link function\n",
      "- the distribution type of the response\n",
      "\n",
      "Unlike linear least squares there is no direct method for evaluating parameter estimates, such as the maximum likelihood estimates (mle's), in a GLM.  We must use iterative methods.  Below we describe the _iteratively reweighted least squares_ (IRLS) algorithm for determining the mle's."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Defining Julia types for GLMs"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The implementation in the [GLM package](https://github.com/JuliaStats/GLM.jl) is patterned after that in [R](http://www.r-project.org).  A model is defined by a formula, a data frame in which to evaluate the identifiers in the formula, a distribution name and the name of the link, which can be omitted when using the canonical link.\n",
      "\n",
      "Concrete types that specialize the abstract `Link` type should define methods for the scalar link function, the scalar inverse link function, and the derivative, $\\frac{d\\mu}{d\\eta}$ as a function of $\\eta$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "using Distributions\n",
      "abstract Link\n",
      "\n",
      "type CauchitLink <: Link end\n",
      "type CloglogLink  <: Link end\n",
      "type IdentityLink <: Link end\n",
      "type InverseLink  <: Link end\n",
      "type LogitLink <: Link end\n",
      "type LogLink <: Link end\n",
      "type ProbitLink <: Link end\n",
      "type SqrtLink <: Link end\n",
      "\n",
      "two(x::FloatingPoint) = one(x) + one(x)\n",
      "half(x::FloatingPoint) = inv(two(x))\n",
      "\n",
      "link(::Type{CauchitLink},\u03bc::FloatingPoint) = tan(pi*(\u03bc - half(\u03bc)))\n",
      "linkinv(::Type{CauchitLink},\u03b7::FloatingPoint) = half(\u03b7) + atan(\u03b7)/\u03c0\n",
      "d\u03bcd\u03b7(::Type{CauchitLink},\u03b7::FloatingPoint) = inv(\u03c0*(one(\u03b7) + abs2(\u03b7)))\n",
      "\n",
      "link(::Type{CloglogLink},\u03bc::FloatingPoint) = log(-log(one(\u03bc) - \u03bc))\n",
      "linkinv(::Type{CloglogLink},\u03b7::FloatingPoint) = -expm1(-exp(\u03b7))\n",
      "d\u03bcd\u03b7(::Type{CloglogLink},\u03b7::FloatingPoint) = exp(\u03b7)*exp(-exp(\u03b7))\n",
      "\n",
      "link(::Type{IdentityLink},\u03bc::FloatingPoint) = \u03bc\n",
      "linkinv(::Type{IdentityLink},\u03b7::FloatingPoint) = \u03b7\n",
      "d\u03bcd\u03b7(::Type{IdentityLink},\u03bc::FloatingPoint) = one(\u03bc)\n",
      "\n",
      "link(::Type{InverseLink},\u03bc::FloatingPoint) = inv(\u03bc)\n",
      "linkinv(::Type{InverseLink},\u03b7::FloatingPoint) = inv(\u03b7)\n",
      "d\u03bcd\u03b7(::Type{InverseLink},\u03b7::FloatingPoint) = -inv(abs2(\u03b7))\n",
      "\n",
      "link(::Type{LogitLink},\u03bc::FloatingPoint) = log(\u03bc/(one(\u03bc)-\u03bc))\n",
      "linkinv(::Type{LogitLink},\u03b7::FloatingPoint) = inv(one(\u03b7) + exp(-\u03b7))\n",
      "d\u03bcd\u03b7(::Type{LogitLink},\u03b7::FloatingPoint) = (e = exp(-abs(\u03b7)); e/abs2(one(e)+e))\n",
      "\n",
      "link(::Type{LogLink},\u03bc::FloatingPoint) = log(\u03bc)\n",
      "linkinv(::Type{LogLink},\u03b7::FloatingPoint) = exp(\u03b7)\n",
      "d\u03bcd\u03b7(::Type{LogLink},\u03b7::FloatingPoint) = exp(\u03b7)\n",
      "\n",
      "link(::Type{ProbitLink},\u03bc::FloatingPoint) = sqrt2*erfinv((two(\u03bc)*\u03bc - one(\u03bc)))\n",
      "linkinv(::Type{ProbitLink},\u03b7::FloatingPoint) = (one(\u03b7) + erf(\u03b7/sqrt2))/two(\u03b7)\n",
      "d\u03bcd\u03b7(::Type{ProbitLink},\u03b7::FloatingPoint) = exp(-abs2(\u03b7)/two(\u03b7))/sqrt2\u03c0\n",
      "\n",
      "link(::Type{SqrtLink},\u03bc::FloatingPoint) = sqrt(\u03bc)\n",
      "linkinv(::Type{SqrtLink},\u03b7::FloatingPoint) = abs2(\u03b7)\n",
      "d\u03bcd\u03b7(::Type{SqrtLink},\u03b7::FloatingPoint) = \u03b7 + \u03b7\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "d\u03bcd\u03b7 (generic function with 8 methods)"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "__Question__: Is it guilding the lily to define functions `two` and `half` instead of using, say, `convert(typeof(\u03bc),0.5)`?  There isn't really a problem with the representation of these values being truncated in that, for example, `big(0.5)` is exact."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "big(0.5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "5e-01 with 256 bits of precision"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "__Question:__ The methods that depend on the Link or the Distribution take the type as the first argument.  I am assuming this will make for effective dispatch if `GLMmodel` is a templated type that includes the distribution and the link as template types."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We restrict the argument to floating point types so that the log's, square roots, etc. retain the same type as the argument.  The math constants `sqrt2` and `sqrt2\u03c0` are defined in the [Distributions package](https://github.com/JuliaStats/Distributions.jl)."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Contributions from the probability distribution"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The probability distribution family defines the variance as a function of the mean, $\\mu$, and also defines the _deviance residuals_.\n",
      "\n",
      "The _deviance_ is defined here as negative twice the log-likelihood setting any scale parameters to unity.  In other words, the deviance depends only on the mean vector, $\\bf\\mu$, which is determined by the linear predictor, $\\bf\\eta$, which is determined by the coefficient vector, $\\bf\\beta$, and the model matrix, $\\bf X$.\n",
      "\n",
      "We also define the variance assuming that any scale parameters are set to one.  It is convenient to define methods for `Base.var` that take the distribution type as the first parameter."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Base.var(::Type{Bernoulli},\u03bc::FloatingPoint) = \u03bc * (one(\u03bc) - \u03bc)\n",
      "Base.var(::Type{Binomial},\u03bc::FloatingPoint) =  \u03bc * (one(\u03bc) - \u03bc)\n",
      "Base.var(::Type{Gamma},\u03bc::FloatingPoint) = abs2(\u03bc)\n",
      "Base.var(::Type{Normal},\u03bc::FloatingPoint) = one(\u03bc)\n",
      "Base.var(::Type{Poisson},\u03bc::FloatingPoint) = \u03bc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "var (generic function with 66 methods)"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "A note about the Binomial distribution."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Generally we consider the response from a Binomial distribution to be the number of successes, $x$, in a fixed number, say $k$, of independent, identical Bernoulli trials.  For the purposes of GLM's we consider the response as the proportion of successes, $x/k$, and use $k$ as the _prior weights_ on the observations. It turns out that a lot of the messy code in implementations of GLMs is there to handle this one specific, but common, case."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Deviance residuals"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In [R](http://www.r-project.org) there is some confusion about what the _deviance residuals_ are.  For a Gaussian distribution the deviance, up to a constant, is the sum of squared residuals.  The _residuals_ are $y_i-\\mu_i,i=1,\\dots,n$.  If we just want to evaluate the deviance then we could use the squared deviance residuals.  For other distributions, it is easier to evaluate the squared deviance residuals, which are the contributions to the deviance from each observation.  (Because the observations are independent, the log-likelihood can always be written as the sum of contributions for each obserations.)\n",
      "\n",
      "To avoid this confusion we call the function that returns the squared deviance residual for an observation, `devresid2`.\n",
      "\n",
      "In R the function that returns the squared deviance residuals is called `devresid`.  If you apply the extractor `resid` or `residuals` with `type=\"deviance\"` you get the signed square root of these squared deviance residuals, leading to some confusion.  (The sign is that of $y_i-\\mu_i$.)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "xlogy{T<:FloatingPoint}(x::T, y::T) = x > zero(T) ? x * log(y) : zero(T)\n",
      "\n",
      "function devresid2{T<:FloatingPoint}(::Type{Bernoulli},y::T,\u03bc::T,wt::T)\n",
      "    omy = one(T) - y\n",
      "    two(y)*wt*(xlogy(y,y/\u03bc) + xlogy(omy,omy/(one(T)-\u03bc)))\n",
      "end\n",
      "function devresid2{T<:FloatingPoint}(::Type{Binomial},y::T,\u03bc::T,wt::T)\n",
      "    devresid2(Bernoulli,y,\u03bc,wt)\n",
      "end\n",
      "function devresid2{T<:FloatingPoint}(::Type{Gamma},y::T,\u03bc::T,wt::T)\n",
      "    -two(y)*wt*(log(y/\u03bc)-(y-\u03bc)/\u03bc)\n",
      "end\n",
      "function devresid2{T<:FloatingPoint}(::Type{Normal},y::T,\u03bc::T,wt::T)\n",
      "    wt * abs2(y-\u03bc)\n",
      "end\n",
      "function devresid2{T<:FloatingPoint}(::Type{Poisson},y::T,\u03bc::T,wt::T)\n",
      "    two(y)*wt*(xlogy(y,y/\u03bc) - (y-\u03bc))\n",
      "end"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "devresid2 (generic function with 5 methods)"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Canonical link functions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "canonicallink(::Type{Bernoulli}) = LogitLink\n",
      "canonicallink(::Type{Binomial}) = LogitLink\n",
      "canonicallink(::Type{Gamma}) = InverseLink\n",
      "canonicallink(::Type{Normal}) = IdentityLink\n",
      "canonicallink(::Type{Poisson}) = LogLink"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "canonicallink (generic function with 5 methods)"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Starting values for \u03bc"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In any iterative algorithm we must determine starting values for some part of the state of the iterations.  In this case it is easiest to determine starting values for $\\mu$ from which starting values for $\\eta$ can be determined using the link function.  When doing this we must avoid the boundary values for $\\mu$, which the link function usually maps to $\\pm\\infty$ on the scale of the linear predictor."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mustart{T<:FloatingPoint}(::Type{Bernoulli},y::T,wt::T) = (wt*y + half(y))/(wt + one(y))\n",
      "mustart{T<:FloatingPoint}(::Type{Binomial},y::T,wt::T) = mustart(Bernoulli,y,wt)\n",
      "mustart{T<:FloatingPoint}(::Type{Gamma},y::T,::T) = y\n",
      "mustart{T<:FloatingPoint}(::Type{Normal},y::T,::T) = y\n",
      "mustart{T<:FloatingPoint}(::Type{Poisson},y::T,::T) = y + inv(convert(typeof(y),10.))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "mustart (generic function with 5 methods)"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Iteratively reweighted least squares (IRLS)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The IRLS algorithm determines $\\hat\\beta$, the mle of the coefficient vector by iterating a process of\n",
      "1. Given the current $\\beta$, determine $\\eta$ and $\\mu$.\n",
      "2. Evaluate the variance (up to a scale factor) from $\\mu$ (function `var`)\n",
      "3. Evaluate case weights from the prior weights, the variance and the derivative, $\\frac{d\\mu}{d\\eta}$ and solve a weighted least squares problem.\n",
      "\n",
      "There are two approaches to the third step - a fixed-point algorithm and an algorithm using a increment.  For both algorithms the _working residual_ is"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wrkresid{T<:FloatingPoint}(y::T,\u03bc::T,\u03bc\u03b7::T) = (y-\u03bc)/\u03bc\u03b7"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "wrkresid (generic function with 1 method)"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For the fixed-point algorithm we create a _working response_ that includes an optional _offset_, `o`, which is occasionally used in a model definition.  In most cases the offset is zero and the prior weights are unity. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wrkresp{T<:FloatingPoint}(wrd::T,\u03b7::T,o::T) = wrd + \u03b7 - o"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "wrkresp (generic function with 1 method)"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The _working weights_ are defined as a function of the prior weight, `w`, the derivative, `\u03bc\u03b7` and the variance, `v`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wrkwt{T<:FloatingPoint}(wt::T,\u03bc\u03b7::T,v::T) = wt*abs2(\u03bc\u03b7/v)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "wrkwt (generic function with 1 method)"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "A `GLMmodel` type"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "At first glance a reasonable design of a user-defined type for GLMs would include vectors `y`, `\u03b7`, `\u03b7`$,\\dots$  However, I could not make this work conveniently if the members of the type are declared as `SharedVector`.  To make it work in general each of these shared vectors would need to have the same set of `pids` associated with it, which would generally be the case but it is not the type of thing you would want to assume without checking.  To avoid this complexity I incorporate all these vectors into a, possibly shared, matrix.  Each column corresponds to an observation so that evaluation (and distribution of the calculations to processes) is performed down columns."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "type GLMmodel{T<:FloatingPoint,D<:Distribution,L<:Link}\n",
      "    X::DenseMatrix{T}        # model matrix\n",
      "    vv::DenseMatrix{T}       # rows are y,o,wt,\u03bc,\u03b7,\u03bc\u03b7,dr,wrsd,wrsp,wwt\n",
      "    \u03b2::DenseVector{T}        # coefficient vector\n",
      "    \u03b4\u03b2::DenseVector{T}       # increment\n",
      "end"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    }
   ],
   "metadata": {}
  }
 ]
}